<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>研究 on 2026 AIA台灣人工智慧學校分享平台</title>
    <link>https://taiwan-ai-academy.github.io/categories/%E7%A0%94%E7%A9%B6/</link>
    <description>Recent content in 研究 on 2026 AIA台灣人工智慧學校分享平台</description>
    <generator>Hugo</generator>
    <language>zh-tw</language>
    <lastBuildDate>Mon, 26 Jan 2026 15:33:45 +0800</lastBuildDate>
    <atom:link href="https://taiwan-ai-academy.github.io/categories/%E7%A0%94%E7%A9%B6/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>誰說 AI 訓練完就定型了？這篇論文讓模型「邊考邊學」直接刷榜</title>
      <link>https://taiwan-ai-academy.github.io/posts/ttt-discover-test-time-training/</link>
      <pubDate>Mon, 26 Jan 2026 15:33:45 +0800</pubDate>
      <guid>https://taiwan-ai-academy.github.io/posts/ttt-discover-test-time-training/</guid>
      <description>&lt;p&gt;以前大家總覺得 LLM 訓練完就是「唯讀」狀態，要解難題只能靠 Prompt Engineering，或是讓模型多想一下。&#xA;但 Stanford 和 NVIDIA 合作的 TTT-Discover 直接打破這個框架：人類遇到難題會試錯、學習、再嘗試；那為什麼 AI 在解題時不能繼續訓練？&lt;/p&gt;</description>
    </item>
    <item>
      <title>SciSciGPT AI 協作工具在《Nature Computational Science》亮相</title>
      <link>https://taiwan-ai-academy.github.io/posts/sciscigpt-nature-computational-science/</link>
      <pubDate>Tue, 20 Jan 2026 17:15:35 +0800</pubDate>
      <guid>https://taiwan-ai-academy.github.io/posts/sciscigpt-nature-computational-science/</guid>
      <description>&lt;p&gt;科研數據這麼複雜，專家分工又太細，能不能開發出一種 AI，讓研究人員不需要變成數據工程師，也能輕鬆搞定研究流程？&lt;/p&gt;</description>
    </item>
    <item>
      <title>OpenAI 研究人員介紹一種訓練 Weight-Sparse Transformers 的典範</title>
      <link>https://taiwan-ai-academy.github.io/posts/openai-weight-sparse-transformers/</link>
      <pubDate>Thu, 27 Nov 2025 09:00:00 +0800</pubDate>
      <guid>https://taiwan-ai-academy.github.io/posts/openai-weight-sparse-transformers/</guid>
      <description>&lt;p&gt;在可解釋性領域中，一個核心目標是找出大型語言模型（LLM）中，人類可以理解的內部計算迴路。OpenAI 研究人員提出的 weight-sparse Transformer 典範，嘗試在訓練階段就強制模型形成結構化的稀疏權重，讓路徑與子模組的功能更可視、可解釋。具體作法是訓練一種權重稀疏化的 Transformer，使絕大多數權重為零；這樣可簡化計算，並引導模型把概念表示分散到多個殘差通道，最終得到較可解釋的計算迴路。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
